{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/yeonsoo/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from bertopic import BERTopic\n",
    "from hdbscan import HDBSCAN\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import spacy\n",
    "import string\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from gensim import corpora, models, utils\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim.models.coherencemodel import CoherenceModel \n",
    "from gensim.parsing.preprocessing import STOPWORDS, remove_stopwords\n",
    "from umap import UMAP\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11231\n",
      "11231\n",
      "11231\n",
      "Many decision makers operate in dynamic environments in which markets, competitors, and technology change regularly. The ability to detect and respond to these regime shifts is critical for economic success. We conduct three experiments to test how effective individuals are at detecting such regime shifts. Specifically, we investigate when individuals are most likely to underreact to change and when they are most likely to overreact to it. We develop a system-neglect hypothesis: Individuals react primarily to the signals they observe and secondarily to the environmental system that produced the signal. The experiments, two involving probability estimation and one involving prediction, reveal a behavioral pattern consistent with our system-neglect hypothesis: Underreaction is most common in unstable environments with precise signals, and overreaction is most common in stable environments with noisy signals. We test this pattern formally in a statistical comparison of the Bayesian model with a parametric specification of the system-neglect model.\n",
      "['many', 'decision', 'maker', 'operate', 'in', 'dynamic', 'environment', 'in', 'which', 'market', 'competitor', 'and', 'technology', 'change', 'regularly', 'the', 'ability', 'to', 'detect', 'and', 'respond', 'to', 'these', 'regime', 'shift', 'be', 'critical', 'for', 'economic', 'success', 'we', 'conduct', 'three', 'experiment', 'to', 'test', 'how', 'effective', 'individual', 'be', 'at', 'detect', 'such', 'regime', 'shift', 'specifically', 'we', 'investigate', 'when', 'individual', 'be', 'most', 'likely', 'to', 'underreact', 'to', 'change', 'and', 'when', 'they', 'be', 'most', 'likely', 'to', 'overreact', 'to', 'it', 'we', 'develop', 'a', 'system', 'neglect', 'hypothesis', 'individual', 'react', 'primarily', 'to', 'the', 'signal', 'they', 'observe', 'and', 'secondarily', 'to', 'the', 'environmental', 'system', 'that', 'produce', 'the', 'signal', 'the', 'experiment', 'two', 'involve', 'probability', 'estimation', 'and', 'one', 'involve', 'prediction', 'reveal', 'a', 'behavioral', 'pattern', 'consistent', 'with', 'our', 'system', 'neglect', 'hypothesis', 'underreaction', 'be', 'most', 'common', 'in', 'unstable', 'environment', 'with', 'precise', 'signal', 'and', 'overreaction', 'be', 'most', 'common', 'in', 'stable', 'environment', 'with', 'noisy', 'signal', 'we', 'test', 'this', 'pattern', 'formally', 'in', 'a', 'statistical', 'comparison', 'of', 'the', 'bayesian', 'model', 'with', 'a', 'parametric', 'specification', 'of', 'the', 'system', 'neglect', 'model']384\n",
      "11231\n",
      "['decision', 'maker', 'operate', 'dynamic', 'environment', 'market', 'competitor', 'technology', 'change', 'regularly', 'ability', 'detect', 'respond', 'regime', 'shift', 'critical', 'economic', 'success', 'conduct', 'experiment', 'effective', 'individual', 'detect', 'regime', 'shift', 'individual', 'likely', 'underreact', 'change', 'likely', 'overreact', 'develop', 'neglect', 'hypothesis', 'individual', 'react', 'primarily', 'signal', 'observe', 'secondarily', 'environmental', 'produce', 'signal', 'experiment', 'involve', 'probability', 'estimation', 'involve', 'prediction', 'reveal', 'behavioral', 'pattern', 'consistent', 'neglect', 'hypothesis', 'underreaction', 'common', 'unstable', 'environment', 'precise', 'signal', 'overreaction', 'common', 'stable', 'environment', 'noisy', 'signal', 'pattern', 'formally', 'statistical', 'comparison', 'bayesian', 'model', 'parametric', 'specification', 'neglect', 'model']<class 'scipy.sparse._csr.csr_matrix'> (11231, 1264)\n",
      "    ability  able  absence  academic  accept  access  accord  account  \\\n",
      "p0        1     0        0         0       0       0       0        0   \n",
      "p1        0     0        0         0       0       0       0        0   \n",
      "p2        0     0        0         0       0       0       0        0   \n",
      "p3        0     0        0         0       0       0       0        0   \n",
      "p4        0     0        0         0       0       0       0        1   \n",
      "\n",
      "    accounting  accuracy  ...  wide  widely  willingness  win  woman  work  \\\n",
      "p0           0         0  ...     0       0            0    0      0     0   \n",
      "p1           0         0  ...     0       0            0    0      0     1   \n",
      "p2           0         0  ...     0       0            0    0      0     0   \n",
      "p3           0         0  ...     0       0            0    0      0     0   \n",
      "p4           0         0  ...     0       0            0    0      0     0   \n",
      "\n",
      "    worker  workplace  world  yield  \n",
      "p0       0          0      0      0  \n",
      "p1       0          0      0      0  \n",
      "p2       0          0      0      0  \n",
      "p3       0          0      0      0  \n",
      "p4       0          0      0      0  \n",
      "\n",
      "[5 rows x 1264 columns]\n",
      "43 ['decision' 'maker' 'operate' 'dynamic' 'environment' 'market'\n",
      " 'competitor' 'technology' 'change' 'ability' 'respond' 'regime' 'shift'\n",
      " 'critical' 'economic' 'success' 'conduct' 'experiment' 'effective'\n",
      " 'individual' 'likely' 'develop' 'hypothesis' 'react' 'primarily' 'signal'\n",
      " 'observe' 'environmental' 'produce' 'involve' 'probability' 'estimation'\n",
      " 'prediction' 'reveal' 'behavioral' 'pattern' 'consistent' 'common'\n",
      " 'stable' 'statistical' 'comparison' 'model' 'decision maker']\n",
      "11231\n",
      "[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 1), (12, 1), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1), (19, 1), (20, 1), (21, 1), (22, 1), (23, 1), (24, 1), (25, 1), (26, 1), (27, 1), (28, 1), (29, 1), (30, 1), (31, 1), (32, 1), (33, 1), (34, 1), (35, 1), (36, 1), (37, 1), (38, 1), (39, 1), (40, 1), (41, 1), (42, 1)] "
     ]
    }
   ],
   "source": [
    "NLP_finn = pd.read_csv(os.path.join(\"..\", \"Project2\", \"data\", \"agg_article_info.csv\"))\n",
    "\n",
    "# Creating a corpus with all abstracts\n",
    "corpus = [NLP_finn['Abst'][i] for i in range(len(NLP_finn['Abst']))]\n",
    "        \n",
    "# Cleaning the unnecessary terms and creating a cleaned corpus\n",
    "corpusn = [i.replace('Research Summary','').replace('Research Abstract','').replace('Research summary','') for i in corpus]\n",
    "\n",
    "# Modified code of Junki Hong's original code\n",
    "corpusnn = []\n",
    "for i in corpusn:\n",
    "    sentences = sent_tokenize(i)\n",
    "    for j in sentences:\n",
    "        if 'Copyright (' in j:\n",
    "            sentences.remove(j)\n",
    "    cleanedAbst = ' '.join(sentences)\n",
    "    corpusnn.append(cleanedAbst)\n",
    "    \n",
    "### Removing punctuations\n",
    "corpus_P = [i.translate(str.maketrans(string.punctuation, ' '*len(string.punctuation))).replace(' '*4, ' ').replace(' '*3, ' ').replace(' '*2, ' ').strip() for i in corpusnn]\n",
    "\n",
    "### Removing numbers\n",
    "# removing numbers: N\n",
    "corpus_PN = [i.translate(str.maketrans('','',string.digits)) for i in corpus_P]\n",
    "# lowercasing: L\n",
    "corpus_PNL = [i.lower() for i in corpus_PN]\n",
    "\n",
    "### Lemmatizing (M) the bill with spaCy library instead of stemming\n",
    "# Use the text before tokenizing; a string need to be provided. spacy does both tokening and lemmatizing.\n",
    "# It takes 4 to 5 minutes and increase nlp.max length to the length set above.\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.max_length = 1000000\n",
    "corpus_PNLM = [[j.lemma_ for j in nlp(i)] for i in corpus_PNL]\n",
    "abst_wclm = [len(i) for i in corpus_PNLM]\n",
    "print(len(abst_wclm))\n",
    "\n",
    "# Removing empty strings from the results of lemmatizing\n",
    "corpus_PNLMf = [' '.join(i).split() for i in corpus_PNLM]\n",
    "print(len(corpus_PNLMf))\n",
    "bill_wclms = [len(i) for i in corpus_PNLMf]\n",
    "print(len(bill_wclms))\n",
    "\n",
    "print(corpus[0])\n",
    "print(corpus_PNLMf[0], end=\"\")\n",
    "\n",
    "# Stopword removal with gensim Library: W\n",
    "# This applied to the lemmatized words! Not to the stemmed words.\n",
    "all_stopwords_add = STOPWORDS.union(set(['x', 'y', 'I', 's', 'e', 'study', 'paper', 'research', 'study', 'literatures', 'article', 'ii', 'iii', 'john',\n",
    "                                        'wiley', 'sons', 'use', 'examine', 'investigate', 'approach', 'argue', 'effect', 'positive', 'negative', 'result',\n",
    "                                        'high', 'increase', 'subsequent', 'subsequently', 'r', 'r d', 'specific', 'specifically', 'think', 'test',\n",
    "                                        'substantially', 'robust', 'second', 'report', 'zero', 'aaa', 'z', 'ab','abd', 'yes', 'year', 'writing', 'd', 'r']))\n",
    "print(len(all_stopwords_add))\n",
    "corpus_PNLMW = [[j for j in i if not j in all_stopwords_add] for i in corpus_PNLMf]\n",
    "corpus_PNLMWs = [' '.join(i) for i in corpus_PNLMW]\n",
    "\n",
    "# Checking if stopword removal reduced words in each bill\n",
    "print(len(corpus_PNLMW))\n",
    "print(corpus_PNLMW[0], end = '')\n",
    "bill_wc1 = [len(i) for i in corpus_PNLMW]\n",
    "\n",
    "### latent semantic analysis (LSA) with scikit-learn\n",
    "# Making document title for each bill\n",
    "P_names = ['p' + str(i) for i in range(len(corpus_PNLMW))]\n",
    "\n",
    "# Making document-term frequency matrix\n",
    "def feed(wordlist): # Use this for feeding preprocessed tokens\n",
    "    return wordlist\n",
    "\n",
    "dtm_md = CountVectorizer(tokenizer = feed, min_df = 0.01, max_df = 0.7, ngram_range = (1,3), token_pattern = None, lowercase = False)\n",
    "dtfm = dtm_md.fit_transform(corpus_PNLMW)\n",
    "print(type(dtfm), dtfm.shape)\n",
    "\n",
    "dtfmx = pd.DataFrame(dtfm.toarray(), index= P_names, columns= dtm_md.get_feature_names_out())\n",
    "print(dtfmx.head())\n",
    "\n",
    "# DTM to DTL by document\n",
    "fin_words = dtm_md.inverse_transform(dtfm)\n",
    "print(len(fin_words[0]), fin_words[0])\n",
    "# Mapping between unique words and word id (making a dictionary)\n",
    "pp_dict = Dictionary(fin_words)\n",
    "# Given a dictionary (pp_dict), make a word frequency table for each document.\n",
    "wordfreq_doc = [pp_dict.doc2bow(text) for text in fin_words] #corpus_PNLMW]\n",
    "print(len(wordfreq_doc))\n",
    "print(wordfreq_doc[0], end = \" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_topics = 50\n",
    "n_words = 10\n",
    "topics = ['topic'+ str(i+1) for i in range(n_topics)]\n",
    "pp_LSA_sklm = TruncatedSVD(n_components= n_topics, n_iter=7, random_state=42)\n",
    "pp_LSA_dtm = pp_LSA_sklm.fit_transform(dtfm)\n",
    "pp_topictermMtx = pd.DataFrame(pp_LSA_sklm.components_, index = topics, columns= dtm_md.get_feature_names_out())\n",
    "idx_nmax_rf = [abs(row).nlargest(n_words).index for index, row in pp_topictermMtx.iterrows()]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldamodel = models.ldamodel.LdaModel(corpus=wordfreq_doc, num_topics= n_topics, id2word= pp_dict, passes= 15,\n",
    "                                    alpha='auto', update_every=1, chunksize=64, random_state=100)\n",
    "coherencemode1 = CoherenceModel(model= ldamodel, texts=fin_words, dictionary=pp_dict, coherence= 'c_v')\n",
    "ldaTopics = ldamodel.print_topics(num_topics= n_topics, num_words = n_words)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERTopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57909fb3f57d43099091e9f53942a011",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/351 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-30 02:23:13,545 - BERTopic - Transformed documents to Embeddings\n",
      "2023-04-30 02:23:16,833 - BERTopic - Reduced dimensionality\n",
      "2023-04-30 02:23:22,581 - BERTopic - Clustered reduced embeddings\n",
      "2023-04-30 02:23:25,679 - BERTopic - Reduced number of topics from 147 to 50\n"
     ]
    }
   ],
   "source": [
    "topic_model = BERTopic(language=\"english\", calculate_probabilities=True, verbose=True, top_n_words=n_words, nr_topics=n_topics)\n",
    "topics, probs = topic_model.fit_transform(corpus_PNLMWs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomly pick five topics per model\n",
    "idx_lsa = np.random.choice(n_topics, 5, replace=False)\n",
    "idx_lda = np.random.choice(n_topics, 5, replace=False)\n",
    "idx_BERTopic = np.random.choice(n_topics, 5, replace=False)\n",
    "\n",
    "topics = []\n",
    "for name in ['LSA', 'LDA', 'BERTopic']:\n",
    "    for i in range(5):\n",
    "        topics.append(name+ str(i+1))\n",
    "topic_lsa = pd.DataFrame(idx_nmax_rf).loc[idx_lsa].T #LSA\n",
    "topic_lda = pd.DataFrame([re.sub(r'\\d+', '', ldaTopics[i][1].replace(\"*\", \"\").replace('\"', '').replace('.', '').replace(' ', '')).split('+')  for i in idx_lda]).replace('\\d+', '').T #LDA\n",
    "topic_BERTopic = pd.concat([pd.DataFrame(topic_model.get_topic(i))[0] for i in idx_BERTopic], axis=1)# BERTopic\n",
    "df = pd.concat([topic_lsa, topic_lda, topic_BERTopic], axis=1)\n",
    "df.columns = topics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LSA1</th>\n",
       "      <th>LSA2</th>\n",
       "      <th>LSA3</th>\n",
       "      <th>LSA4</th>\n",
       "      <th>LSA5</th>\n",
       "      <th>LDA1</th>\n",
       "      <th>LDA2</th>\n",
       "      <th>LDA3</th>\n",
       "      <th>LDA4</th>\n",
       "      <th>LDA5</th>\n",
       "      <th>BERTopic1</th>\n",
       "      <th>BERTopic2</th>\n",
       "      <th>BERTopic3</th>\n",
       "      <th>BERTopic4</th>\n",
       "      <th>BERTopic5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>status</td>\n",
       "      <td>technology</td>\n",
       "      <td>strategic</td>\n",
       "      <td>status</td>\n",
       "      <td>ceo</td>\n",
       "      <td>external</td>\n",
       "      <td>shape</td>\n",
       "      <td>interaction</td>\n",
       "      <td>drive</td>\n",
       "      <td>experience</td>\n",
       "      <td>diffusion</td>\n",
       "      <td>flexibility</td>\n",
       "      <td>leadership</td>\n",
       "      <td>family</td>\n",
       "      <td>patch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>technology</td>\n",
       "      <td>manager</td>\n",
       "      <td>platform</td>\n",
       "      <td>theory</td>\n",
       "      <td>board</td>\n",
       "      <td>supplier</td>\n",
       "      <td>future</td>\n",
       "      <td>appropriate</td>\n",
       "      <td>significantly</td>\n",
       "      <td>question</td>\n",
       "      <td>adoption</td>\n",
       "      <td>flexible</td>\n",
       "      <td>leader</td>\n",
       "      <td>ownership</td>\n",
       "      <td>vendor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>policy</td>\n",
       "      <td>consumer</td>\n",
       "      <td>strategy</td>\n",
       "      <td>organization</td>\n",
       "      <td>group</td>\n",
       "      <td>component</td>\n",
       "      <td>diversity</td>\n",
       "      <td>configuration</td>\n",
       "      <td>complementary</td>\n",
       "      <td>participate</td>\n",
       "      <td>innovation</td>\n",
       "      <td>plant</td>\n",
       "      <td>abusive</td>\n",
       "      <td>divestiture</td>\n",
       "      <td>security</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>supplier</td>\n",
       "      <td>relationship</td>\n",
       "      <td>business</td>\n",
       "      <td>strategy</td>\n",
       "      <td>risk</td>\n",
       "      <td>failure</td>\n",
       "      <td>basis</td>\n",
       "      <td>raise</td>\n",
       "      <td>especially</td>\n",
       "      <td>win</td>\n",
       "      <td>internet</td>\n",
       "      <td>product</td>\n",
       "      <td>supervision</td>\n",
       "      <td>firm</td>\n",
       "      <td>software</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>new</td>\n",
       "      <td>theory</td>\n",
       "      <td>experience</td>\n",
       "      <td>consumer</td>\n",
       "      <td>new</td>\n",
       "      <td>responsibility</td>\n",
       "      <td>direction</td>\n",
       "      <td>signal</td>\n",
       "      <td>capture</td>\n",
       "      <td>transactioncost</td>\n",
       "      <td>adopter</td>\n",
       "      <td>resource</td>\n",
       "      <td>team</td>\n",
       "      <td>owner</td>\n",
       "      <td>vulnerability</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>experience</td>\n",
       "      <td>contract</td>\n",
       "      <td>relationship</td>\n",
       "      <td>policy</td>\n",
       "      <td>change</td>\n",
       "      <td>buyer</td>\n",
       "      <td>imply</td>\n",
       "      <td>fact</td>\n",
       "      <td>innovative</td>\n",
       "      <td>power</td>\n",
       "      <td>practice</td>\n",
       "      <td>chain</td>\n",
       "      <td>supervisor</td>\n",
       "      <td>performance</td>\n",
       "      <td>liability</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>time</td>\n",
       "      <td>innovation</td>\n",
       "      <td>industry</td>\n",
       "      <td>search</td>\n",
       "      <td>employee</td>\n",
       "      <td>hazard</td>\n",
       "      <td>ignore</td>\n",
       "      <td>world</td>\n",
       "      <td>respect</td>\n",
       "      <td>yield</td>\n",
       "      <td>bandwagon</td>\n",
       "      <td>manufacturing</td>\n",
       "      <td>transformational</td>\n",
       "      <td>non</td>\n",
       "      <td>release</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>buyer</td>\n",
       "      <td>supplier</td>\n",
       "      <td>change</td>\n",
       "      <td>quality</td>\n",
       "      <td>model</td>\n",
       "      <td>criterion</td>\n",
       "      <td>actually</td>\n",
       "      <td>induce</td>\n",
       "      <td>timing</td>\n",
       "      <td>multilevel</td>\n",
       "      <td>network</td>\n",
       "      <td>configuration</td>\n",
       "      <td>subordinate</td>\n",
       "      <td>control</td>\n",
       "      <td>ransom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>retailer</td>\n",
       "      <td>new</td>\n",
       "      <td>contract</td>\n",
       "      <td>social</td>\n",
       "      <td>director</td>\n",
       "      <td>meet</td>\n",
       "      <td>regulation</td>\n",
       "      <td>reputation</td>\n",
       "      <td>quantity</td>\n",
       "      <td>horizon</td>\n",
       "      <td>iso</td>\n",
       "      <td>budget</td>\n",
       "      <td>follower</td>\n",
       "      <td>agency</td>\n",
       "      <td>attack</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>process</td>\n",
       "      <td>process</td>\n",
       "      <td>quality</td>\n",
       "      <td>time</td>\n",
       "      <td>decision</td>\n",
       "      <td>sufficiently</td>\n",
       "      <td>consequently</td>\n",
       "      <td>game</td>\n",
       "      <td>horizon</td>\n",
       "      <td>difficult</td>\n",
       "      <td>adopt</td>\n",
       "      <td>production</td>\n",
       "      <td>behavior</td>\n",
       "      <td>shareholder</td>\n",
       "      <td>ransomware</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         LSA1          LSA2          LSA3          LSA4      LSA5  \\\n",
       "0      status    technology     strategic        status       ceo   \n",
       "1  technology       manager      platform        theory     board   \n",
       "2      policy      consumer      strategy  organization     group   \n",
       "3    supplier  relationship      business      strategy      risk   \n",
       "4         new        theory    experience      consumer       new   \n",
       "5  experience      contract  relationship        policy    change   \n",
       "6        time    innovation      industry        search  employee   \n",
       "7       buyer      supplier        change       quality     model   \n",
       "8    retailer           new      contract        social  director   \n",
       "9     process       process       quality          time  decision   \n",
       "\n",
       "             LDA1          LDA2           LDA3           LDA4  \\\n",
       "0        external         shape    interaction          drive   \n",
       "1        supplier        future    appropriate  significantly   \n",
       "2       component     diversity  configuration  complementary   \n",
       "3         failure         basis          raise     especially   \n",
       "4  responsibility     direction         signal        capture   \n",
       "5           buyer         imply           fact     innovative   \n",
       "6          hazard        ignore          world        respect   \n",
       "7       criterion      actually         induce         timing   \n",
       "8            meet    regulation     reputation       quantity   \n",
       "9    sufficiently  consequently           game        horizon   \n",
       "\n",
       "              LDA5   BERTopic1      BERTopic2         BERTopic3    BERTopic4  \\\n",
       "0       experience   diffusion    flexibility        leadership       family   \n",
       "1         question    adoption       flexible            leader    ownership   \n",
       "2      participate  innovation          plant           abusive  divestiture   \n",
       "3              win    internet        product       supervision         firm   \n",
       "4  transactioncost     adopter       resource              team        owner   \n",
       "5            power    practice          chain        supervisor  performance   \n",
       "6            yield   bandwagon  manufacturing  transformational          non   \n",
       "7       multilevel     network  configuration       subordinate      control   \n",
       "8          horizon         iso         budget          follower       agency   \n",
       "9        difficult       adopt     production          behavior  shareholder   \n",
       "\n",
       "       BERTopic5  \n",
       "0          patch  \n",
       "1         vendor  \n",
       "2       security  \n",
       "3       software  \n",
       "4  vulnerability  \n",
       "5      liability  \n",
       "6        release  \n",
       "7         ransom  \n",
       "8         attack  \n",
       "9     ransomware  "
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
